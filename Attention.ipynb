{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ea99d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# dùng executing_eagerly mode trong tensor    \n",
    "tf.executing_eagerly()\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sent = sentence.lower()\n",
    "    sent = sent.strip()\n",
    "    sent = re.sub(\"'\", \" \", sent)\n",
    "    sent = re.sub(\"\\s+\", \" \", sent)\n",
    "    sent = ''.join([char for char in sent if char not in exclude])\n",
    "    sent = \"<start> \" + sent + \" <end>\"\n",
    "    return sent\n",
    "    \n",
    "# load data\n",
    "en_filename = \"./dataset/train.en.txt\"\n",
    "vi_filename = \"./dataset/train.vi.txt\"\n",
    "raw_en_lines = open(en_filename, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "raw_vi_lines = open(vi_filename, encoding='utf-8').read().strip().split(\"\\n\")\n",
    "exclude = list(string.punctuation) + list(string.digits)\n",
    "\n",
    "en_lines = []\n",
    "vi_lines = []\n",
    "min_len, max_len = 10, 14\n",
    "\n",
    "\n",
    "# loại bỏ những câu có độ dài không nằm trong khoảng (min_len, max_len)\n",
    "for eline, vline in zip(raw_en_lines, raw_vi_lines):\n",
    "    eline = preprocess(eline)\n",
    "    vline = preprocess(vline)\n",
    "    if min_len < len(eline.split(\" \")) < max_len and min_len < len(vline.split(\" \")) < max_len:\n",
    "        en_lines.append(eline)\n",
    "        vi_lines.append(vline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6512f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle Languages\n",
    "\n",
    "class Language():\n",
    "    def __init__(self, lines):\n",
    "        self.lines = lines\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.vocab = set()\n",
    "        self.max_len = 0\n",
    "        self.min_len = 0\n",
    "        self.vocab_size = 0\n",
    "        self.init_language_params()\n",
    "\n",
    "    def init_language_params(self):\n",
    "        self.word2id['<pad>'] = 0\n",
    "        for line in self.lines:\n",
    "            self.vocab.update(line.split(\" \"))\n",
    "        for id, word in enumerate(self.vocab):\n",
    "            self.word2id[word] = id + 1\n",
    "        for word, id in self.word2id.items():\n",
    "            self.id2word[id] = word\n",
    "        \n",
    "        self.max_len = max([len(line.split(\" \")) for line in self.lines])\n",
    "        self.min_len = min([len(line.split(\" \")) for line in self.lines])\n",
    "        self.vocab_size = len(self.vocab) + 1\n",
    "            \n",
    "    def sentence_to_vector(self, sent):\n",
    "        return np.array([self.word2id[word] for word in sent.split(\" \")])\n",
    "            \n",
    "    def vector_to_sentence(self, vector):\n",
    "        return \" \".join([self.id2word[id] for id in vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6203fb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 15:18:16.697016: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-10 15:18:16.697968: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Tạo dữ liệu cho train và test\n",
    "\n",
    "inp_lang = Language(en_lines)\n",
    "tar_lang = Language(vi_lines)\n",
    "inp_vector = [inp_lang.sentence_to_vector(line) for line in inp_lang.lines]\n",
    "tar_vector = [tar_lang.sentence_to_vector(line) for line in tar_lang.lines]\n",
    "\n",
    "# add padding vào câu để tất cả các câu có độ dài bằng nhau\n",
    "inp_tensor = tf.keras.preprocessing.sequence.pad_sequences(inp_vector, inp_lang.max_len, padding='post')\n",
    "tar_tensor = tf.keras.preprocessing.sequence.pad_sequences(tar_vector, tar_lang.max_len, padding='post')\n",
    "x_train, x_val, y_train, y_val = train_test_split(inp_tensor, tar_tensor, test_size=0.1)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = x_train.shape[0]\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "hidden_unit = 1024\n",
    "embedding_size = 256\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87f06b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encode(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, vocab_size, hidden_units):\n",
    "        super(Encode, self).__init__()\n",
    "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.GRU = tf.keras.layers.GRU(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform')\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "    def call(self, x, hidden_state):\n",
    "        x = self.Embedding(x)\n",
    "        outputs, last_state = self.GRU(x, hidden_state)\n",
    "        return outputs, last_state\n",
    "    \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        return tf.zeros([batch_size, self.hidden_units])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab80464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.Model):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W_out_encode = tf.keras.layers.Dense(hidden_unit)\n",
    "        self.W_state = tf.keras.layers.Dense(hidden_unit)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, encode_outs, pre_state):\n",
    "        pre_state = tf.expand_dims(pre_state, axis=1)\n",
    "        pre_state = self.W_state(pre_state)\n",
    "        encode_outs = self.W_out_encode(encode_outs)\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                pre_state + encode_outs)\n",
    "        )\n",
    "        score = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = score*encode_outs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a61879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decode(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_units):\n",
    "        super(Decode, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.Embedding = tf.keras.layers.Embedding(vocab_size,embedding_size)\n",
    "        self.Attention = Attention(hidden_units)\n",
    "        self.GRU = tf.keras.layers.GRU(\n",
    "            hidden_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.Fc = tf.keras.layers.Dense(vocab_size)\n",
    "            \n",
    "    def call(self, x, encode_outs, pre_state):\n",
    "        x = tf.expand_dims(x, axis=1)\n",
    "        x = self.Embedding(x)\n",
    "        context_vector, attention_weight = self.Attention(encode_outs, pre_state)\n",
    "        context_vector = tf.expand_dims(context_vector, axis=1)\n",
    "        gru_inp = tf.concat([x, context_vector], axis=-1)\n",
    "        out_gru, state = self.GRU(gru_inp)\n",
    "        out_gru = tf.reshape(out_gru, (-1, out_gru.shape[2]))\n",
    "        return self.Fc(out_gru), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109d1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9619bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570.779\n",
      "9102.253\n",
      "8161.6494\n",
      "7346.4644\n",
      "6682.1133\n",
      "6044.956\n",
      "5434.477\n",
      "4899.714\n",
      "4371.02\n",
      "3899.9282\n",
      "3523.023\n",
      "3196.154\n",
      "2822.7212\n",
      "2419.262\n",
      "2091.5415\n",
      "1836.2429\n",
      "1601.4626\n",
      "1383.8468\n",
      "1181.2268\n",
      "1024.362\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "EPOCHS = 20\n",
    "optimizer = tf.optimizers.Adam()\n",
    "encoder = Encode(embedding_size, vocab_size=inp_lang.vocab_size, hidden_units=hidden_unit)\n",
    "decoder = Decode(vocab_size=tar_lang.vocab_size, embedding_size=embedding_size, hidden_units=hidden_unit)\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_id, (x, y) in enumerate(dataset.take(N_BATCH)):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            first_state = encoder.init_hidden_state(batch_size=BATCH_SIZE)\n",
    "            encode_outs, last_state = encoder(x, first_state)\n",
    "            decode_state = last_state\n",
    "            decode_input = [tar_lang.word2id[\"<start>\"]]*BATCH_SIZE\n",
    "            \n",
    "            for i in range(1, y.shape[1]):\n",
    "                decode_out, decode_state = decoder(\n",
    "                        decode_input, encode_outs, decode_state\n",
    "                )\n",
    "                loss += loss_function(y[:, i], decode_out)\n",
    "                decode_input = y[:, i]\n",
    "                \n",
    "            train_vars = encoder.trainable_variables + decoder.trainable_variables\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "            optimizer.apply_gradients(zip(grads, train_vars))\n",
    "        total_loss += loss\n",
    "    print(total_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77606410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bạn có thấy tôi lấy tôi lấy tôi lấy tôi lấy tôi lấy \n"
     ]
    }
   ],
   "source": [
    "def translate(inputs):\n",
    "    result = ''\n",
    "    hidden = encoder.init_hidden_state(batch_size=1)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden  \n",
    "    dec_input = [tar_lang.word2id['<start>']]\n",
    "    \n",
    "    for t in range(tar_lang.max_len):\n",
    "        predictions, dec_hidden = decoder(dec_input, enc_out, dec_hidden)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += tar_lang.id2word[predicted_id] + ' '\n",
    "        dec_input = [predicted_id]\n",
    "    return result\n",
    "  \n",
    "for inp, tar in dataset.take(N_BATCH):\n",
    "    print(translate(inp[1:2,:]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f77dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
